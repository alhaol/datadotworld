{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# data.world datacamp tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> #### Setup  \n",
    "\n",
    "> Before running data.world notebooks for the first time, you'll need to:  \n",
    "1. Install data.world's Python package, including optional `pandas` dependencies: \n",
    "```shell\n",
    "pip install datadotworld[pandas]\n",
    "```\n",
    "1. Obtain an API access token at https://data.world/settings/advanced\n",
    "1. Store API access token using the `dw` command-line tool: \n",
    "```shell\n",
    "dw configure\n",
    "```\n",
    "\n",
    "3 ) Copy/Paste in your data.world API token when prompted. Find your token at https://data.world/settings/advanced\n",
    "\n",
    "We've also included one final code example here that shows some more ways to inspect a dataset, so check it out and just click submit to finish the course. We hope you enjoyed it!\n",
    "\n",
    "> Once your environment is set up, these steps do not need to be repeated for other data.world notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.0136668165525 min to dowanload 1 miilion AIS records\n"
     ]
    }
   ],
   "source": [
    "# Import the datadotworld module as dw and the sys module\n",
    "import datadotworld as dw\n",
    "import sys\n",
    "import pprint as pp\n",
    "import time \n",
    "\n",
    "Start=time.time()\n",
    "\n",
    "# Import a dataset\n",
    "aisdata = dw.load_dataset('alhaol/ais')\n",
    "\n",
    "print ('It took {0} min to dowanload 1 miilion AIS records'. \\\n",
    "       format(float(time.time()-Start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'dyn1000000', u'stat1000000']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(aisdata.dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a402c3e8776a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mDYN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maisdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dyn1000000'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mSTAT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maisdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stat1000000'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'It took {0} min to dowanload {1} MB DYN and {2} MB STAT '\u001b[0m\u001b[0;34m.\u001b[0m        \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mStart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDYN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTAT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "Start=time.time()\n",
    "DYN=aisdata.dataframes['dyn1000000']\n",
    "STAT=aisdata.dataframes['stat1000000']\n",
    "print ('It took {0} min to dowanload {1} MB DYN and {2} MB STAT '. \\\n",
    "       format(float(time.time()-Start)/60), \\\n",
    "       sys.getsizeof(DYN)/1e6, sys.getsizeof(STAT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DYN.to_csv('dyn1000000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the datadotworld module as dw and the sys module\n",
    "import datadotworld as dw\n",
    "import sys\n",
    "import pprint as pp\n",
    "\n",
    "# Import a dataset\n",
    "refugee_dataset = dw.load_dataset('nrippner/refugee-host-nations')\n",
    "\n",
    "# Get the size of the dataset:\n",
    "print(\" \")\n",
    "print('The Size of the DataSet= {0}'.format(sys.getsizeof(refugee_dataset)))\n",
    "print(\" \")\n",
    "\n",
    "# List all of the data files:\n",
    "dataframes = refugee_dataset.dataframes\n",
    "print(\" \")\n",
    "for df in dataframes:\n",
    "    pp.pprint(df)\n",
    "print(\" \")\n",
    "    \n",
    "# print all of the files in a dataset:\n",
    "resources = refugee_dataset.describe()['resources']\n",
    "pp.pprint('name:')\n",
    "\n",
    "for r in resources:\n",
    "    pp.pprint(r['name'])\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "pp.pprint('/n type of file:')\n",
    "\n",
    "for r in resources:\n",
    "    pp.pprint(r['format'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import datadotworld module and pp.pprint\n",
    "\n",
    "import datadotworld as dw\n",
    "import os\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib for plots\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load_dataset examples\n",
    "\n",
    "dataset = dw.load_dataset('https://data.world/stephen-hoover/chicago-city-council-votes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# describe examples: describe dataset\n",
    "\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# describe examples: describe specific dataset resource\n",
    "\n",
    "dataset.describe('alderman_votes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Accessing the data\n",
    "## After loading a dataset object, you can access the data via: `raw_data`, `tables`, or `dataframes`.\n",
    "## Each of these returns a dictionary of values: `bytes`, `list` and `pandas.DataFrame` objects, respectively.\n",
    "\n",
    "votes_dataframe = dataset.dataframes['alderman_votes']\n",
    "votes_dataframe.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Working with multiple datasets\n",
    "\n",
    "# Load two datasets from data.world that you'd like to merge:\n",
    "int_dataset = dw.load_dataset('https://data.world/jonloyens/intermediate-data-world')\n",
    "fipsCodes_dataset = dw.load_dataset('https://data.world/uscensusbureau/fips-state-codes')\n",
    "\n",
    "# Create two dataframes\n",
    "police_shootings = int_dataset.dataframes['fatal_police_shootings_data']\n",
    "state_abbrvs = fipsCodes_dataset.dataframes['statesfipscodes']\n",
    "\n",
    "## Merge the two datasets together on the state and stusab fields:\n",
    "merged_dataframe = police_shootings.merge(state_abbrvs, how = 'left', left_on = 'state', right_on='stusab')\n",
    "\n",
    "## Create a 'citystate' column in the merged_dataframe dataframe with the format `city, state_name`:\n",
    "merged_dataframe[\"citystate\"] = merged_dataframe[\"city\"] + \", \" + merged_dataframe[\"state_name\"]\n",
    "\n",
    "## Print head of merged dataframe\n",
    "pp.pprint(merged_dataframe.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Querying data.world via SDK\n",
    "## Query using SQL or SPARQL query languages. SQL is default, or add `query_type='sparql'` as a parameter for SPARQL. \n",
    "## The `query()` method gives you access to three properties to access the resulting data: `raw_data`, `table`, and `dataframe`.\n",
    "\n",
    "# SQL\n",
    "\n",
    "## Single table query exercise\n",
    "\n",
    "# Define query string\n",
    "sql_query = \"SELECT * FROM `unhcr_all` WHERE Year = 2010\"\n",
    "\n",
    "# Query table, passing query string as parameter\n",
    "query2010 = dw.query('https://data.world/nrippner/refugee-host-nations', sql_query)\n",
    "\n",
    "# Create dataframe using dataframe property\n",
    "unhcr2010 = query2010.dataframe\n",
    "\n",
    "# Print first 5 rows of results\n",
    "unhcr2010.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# SQL\n",
    "\n",
    "## Multi-table join exercise\n",
    "\n",
    "# Define query string. Note that secondary table and fields are explicitly referenced using dataset key (ownerid/tableid)\n",
    "sql_query = \"SELECT state, count(fmid) as count, Avg(obesity.Value) as obesityAvg FROM Export LEFT JOIN health.`obesity-by-state-2014`.`adult_obese` as obesity ON state = obesity.location GROUP BY state ORDER BY count desc\"\n",
    "\n",
    "# Query 'local' table, passing query string as parameter\n",
    "queryResults = dw.query('https://data.world/agriculture/national-farmers-markets', sql_query)\n",
    "\n",
    "# Create dataframe using dataframe property\n",
    "stateStats = queryResults.dataframe\n",
    "\n",
    "# Plot results on state\n",
    "stateStats.plot(x='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# SPARQL\n",
    "## Learn SPARQL using our tutorial at https://docs.data.world/documentation/api/sparql.html\n",
    "\n",
    "# Define query string\n",
    "sparql_query = \"PREFIX GOT: <https://tutorial.linked.data.world/d/sparqltutorial/> SELECT ?FName ?LName WHERE {?person GOT:col-got-house \\\"Stark\\\" . ?person GOT:col-got-fname ?FName . ?person GOT:col-got-lname ?LName .}\"\n",
    "\n",
    "# Query table, passing query string and `query_type` as parameters\n",
    "queryResults = dw.query('http://data.world/tutorial/sparqltutorial', sparql_query, query_type='sparql')\n",
    "\n",
    "# Create dataframe using dataframe property\n",
    "houseStark = queryResults.dataframe\n",
    "\n",
    "# Print first 5 rows of results\n",
    "pp.pprint(houseStark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Advanced SDK Functionality\n",
    "\n",
    "The data.world Python SDK includes a variety of API wrappers, available via the `ApiClient` class, that let you create, replace, update, and delete a dataset. In this section, we’ll walk through a few common tasks:\n",
    "\n",
    "- Use `api_client()` to get an instance of the `ApiClient`\n",
    "- Create a dataset\n",
    "- Add a file from a dataframe: we’ll write to a local csv and the upload the file\n",
    "- Add a file from a source URL: this is an easy way to add external data to your dataset and keep it up to date. We’ll use a file from GitHub as an example, but you can use any URL source that points to a file.\n",
    "- Sync the dataset: this simple call reloads any files with a source URL, to ensure the latest version.\n",
    "- Update the dataset: after creating a dataset, use `update_dataset` to change attirbutes like description, summary or tags.\n",
    "\n",
    "Use `help(api_client)` to learn more about each available function or see the full [data.world API documentation](https://docs.data.world/documentation/api/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create an instance of the ApiClient using `api_client()`\n",
    "api_client = dw.api_client()\n",
    "\n",
    "# See api_client documentation\n",
    "help(api_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a dataset using create_dataset method. \n",
    "\n",
    "# Replace the < > items with your username and desired dataset title. Visibility can be changed to 'OPEN' if you choose.\n",
    "api_client.create_dataset(owner_id=\"alhaol\", title=\"Test\", visibility='PRIVATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write a dataframe to a local file and upload to dataset\n",
    "\n",
    "# Create dataframe\n",
    "police_shootings = dw.load_dataset('https://data.world/jonloyens/intermediate-data-world').dataframes['fatal_police_shootings_data']\n",
    "\n",
    "# Write dataframe to local csv using pandas to_csv() method\n",
    "police_shootings.to_csv('police_shootings.csv', encoding='utf-8')\n",
    "\n",
    "# Add file to your dataset using upload_files(). Replace the < > items with your dataset values\n",
    "api_client.upload_files('alhaol/test',['police_shootings.csv'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Update dataset\n",
    "\n",
    "# Add a file from an external source URL. In this example we'll use github. \n",
    "# Replace the < > items with your dataset values\n",
    "api_client.add_files_via_url('alhao/test',{'shootings_of_police1.csv': 'https://github.com/fivethirtyeight/data/blob/master/police-deaths/all_data.csv'})\n",
    "\n",
    "# For files added with add_files_via_url, fetch the latest version using the sync() method:\n",
    "api_client.sync_files('alhaol/test')\n",
    "                                                                \n",
    "# Use the update_dataset() method to update the metadata after dataset creation:\n",
    "api_client.update_dataset('alhaol/test', description='Dataset created to test out the python SDK functionality.', tags=['test', 'datacamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
